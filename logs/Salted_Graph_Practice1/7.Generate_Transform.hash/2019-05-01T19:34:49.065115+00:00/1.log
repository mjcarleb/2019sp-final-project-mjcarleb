[2019-05-01 15:35:47,515] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: Salted_Graph_Practice1.7.Generate_Transform.hash 2019-05-01T19:34:49.065115+00:00 [queued]>
[2019-05-01 15:35:47,521] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: Salted_Graph_Practice1.7.Generate_Transform.hash 2019-05-01T19:34:49.065115+00:00 [queued]>
[2019-05-01 15:35:47,521] {__init__.py:1353} INFO - 
--------------------------------------------------------------------------------
[2019-05-01 15:35:47,521] {__init__.py:1354} INFO - Starting attempt 1 of 1
[2019-05-01 15:35:47,521] {__init__.py:1355} INFO - 
--------------------------------------------------------------------------------
[2019-05-01 15:35:47,533] {__init__.py:1374} INFO - Executing <Task(PythonOperator): 7.Generate_Transform.hash> on 2019-05-01T19:34:49.065115+00:00
[2019-05-01 15:35:47,533] {base_task_runner.py:119} INFO - Running: ['airflow', 'run', 'Salted_Graph_Practice1', '7.Generate_Transform.hash', '2019-05-01T19:34:49.065115+00:00', '--job_id', '362', '--raw', '-sd', 'DAGS_FOLDER/salted_graph.py', '--cfg_path', '/tmp/tmpd2mc7qv5']
[2019-05-01 15:35:47,815] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash /home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/configuration.py:214: FutureWarning: The task_runner setting in [core] has the old default value of 'BashTaskRunner'. This value has been changed to 'StandardTaskRunner' in the running config, but please update your config before Apache Airflow 2.0.
[2019-05-01 15:35:47,815] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   FutureWarning,
[2019-05-01 15:35:47,973] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash [2019-05-01 15:35:47,973] {__init__.py:51} INFO - Using executor SequentialExecutor
[2019-05-01 15:35:48,118] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash [2019-05-01 15:35:48,118] {__init__.py:305} INFO - Filling up the DagBag from /home/mjcarleb/HESWork/CSCI-E29/2019sp-airflow_project-mjcarleb/dags/salted_graph.py
[2019-05-01 15:35:48,264] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash [2019-05-01 15:35:48,264] {cli.py:517} INFO - Running <TaskInstance: Salted_Graph_Practice1.7.Generate_Transform.hash 2019-05-01T19:34:49.065115+00:00 [running]> on host localhost.localdomain
[2019-05-01 15:35:48,269] {python_operator.py:104} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=Salted_Graph_Practice1
AIRFLOW_CTX_TASK_ID=7.Generate_Transform.hash
AIRFLOW_CTX_EXECUTION_DATE=2019-05-01T19:34:49.065115+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2019-05-01T19:34:49.065115+00:00
[2019-05-01 15:35:48,402] {logging_mixin.py:95} WARNING - /home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pyarrow/pandas_compat.py:114: FutureWarning: A future version of pandas will default to `skipna=True`. To silence this warning, pass `skipna=True|False` explicitly.
  result = infer_dtype(pandas_collection)
[2019-05-01 15:35:48,402] {__init__.py:1580} ERROR - [Errno 2] No such file or directory: '/home/mjcarleb/HESWork/CSCI-E29/2019sp-airflow_project-mjcarleb/transformed_data/yelp_subset_1.913814c2.parquet'
Traceback (most recent call last):
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 112, in execute
    return_value = self.execute_callable()
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 117, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/mjcarleb/HESWork/CSCI-E29/2019sp-airflow_project-mjcarleb/dags/salted_graph.py", line 254, in transform_data
    df.to_parquet(transformed_file_path)
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pandas/core/frame.py", line 2203, in to_parquet
    partition_cols=partition_cols, **kwargs)
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pandas/io/parquet.py", line 252, in to_parquet
    partition_cols=partition_cols, **kwargs)
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pandas/io/parquet.py", line 122, in write
    coerce_timestamps=coerce_timestamps, **kwargs)
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pyarrow/parquet.py", line 1124, in write_table
    **kwargs) as writer:
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pyarrow/parquet.py", line 340, in __init__
    sink = self.file_handle = fs.open(where, 'wb')
  File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pyarrow/filesystem.py", line 245, in open
    return open(path, mode=mode)
FileNotFoundError: [Errno 2] No such file or directory: '/home/mjcarleb/HESWork/CSCI-E29/2019sp-airflow_project-mjcarleb/transformed_data/yelp_subset_1.913814c2.parquet'
[2019-05-01 15:35:48,404] {__init__.py:1611} INFO - Marking task as FAILED.
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash Traceback (most recent call last):
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/bin/airflow", line 32, in <module>
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     args.func(args)
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     return f(*args, **kwargs)
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/bin/cli.py", line 523, in run
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     _run(args, dag, ti)
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/bin/cli.py", line 442, in _run
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     pool=args.pool,
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/utils/db.py", line 73, in wrapper
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     return func(*args, **kwargs)
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     result = task_copy.execute(context=context)
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 112, in execute
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     return_value = self.execute_callable()
[2019-05-01 15:35:48,416] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 117, in execute_callable
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     return self.python_callable(*self.op_args, **self.op_kwargs)
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/HESWork/CSCI-E29/2019sp-airflow_project-mjcarleb/dags/salted_graph.py", line 254, in transform_data
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     df.to_parquet(transformed_file_path)
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pandas/core/frame.py", line 2203, in to_parquet
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     partition_cols=partition_cols, **kwargs)
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pandas/io/parquet.py", line 252, in to_parquet
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     partition_cols=partition_cols, **kwargs)
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pandas/io/parquet.py", line 122, in write
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     coerce_timestamps=coerce_timestamps, **kwargs)
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pyarrow/parquet.py", line 1124, in write_table
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     **kwargs) as writer:
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pyarrow/parquet.py", line 340, in __init__
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     sink = self.file_handle = fs.open(where, 'wb')
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash   File "/home/mjcarleb/anaconda3/envs/airflow5/lib/python3.7/site-packages/pyarrow/filesystem.py", line 245, in open
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash     return open(path, mode=mode)
[2019-05-01 15:35:48,417] {base_task_runner.py:101} INFO - Job 362: Subtask 7.Generate_Transform.hash FileNotFoundError: [Errno 2] No such file or directory: '/home/mjcarleb/HESWork/CSCI-E29/2019sp-airflow_project-mjcarleb/transformed_data/yelp_subset_1.913814c2.parquet'
[2019-05-01 15:35:52,533] {logging_mixin.py:95} INFO - [2019-05-01 15:35:52,532] {jobs.py:2562} INFO - Task exited with return code 1
