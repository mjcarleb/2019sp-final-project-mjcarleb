[2019-04-29 07:49:35,549] {models.py:1359} INFO - Dependencies all met for <TaskInstance: s3_badcred_sensor.task2 2019-04-28T00:00:00+00:00 [queued]>
[2019-04-29 07:49:35,552] {models.py:1359} INFO - Dependencies all met for <TaskInstance: s3_badcred_sensor.task2 2019-04-28T00:00:00+00:00 [queued]>
[2019-04-29 07:49:35,552] {models.py:1571} INFO - 
--------------------------------------------------------------------------------
Starting attempt 1 of 2
--------------------------------------------------------------------------------

[2019-04-29 07:49:35,561] {models.py:1593} INFO - Executing <Task(BashOperator): task2> on 2019-04-28T00:00:00+00:00
[2019-04-29 07:49:35,561] {base_task_runner.py:118} INFO - Running: ['bash', '-c', 'airflow run s3_badcred_sensor task2 2019-04-28T00:00:00+00:00 --job_id 102 --raw -sd DAGS_FOLDER/AWS_Challenge.py --cfg_path /tmp/tmp70wodsxb']
[2019-04-29 07:49:36,013] {base_task_runner.py:101} INFO - Job 102: Subtask task2 [2019-04-29 07:49:36,012] {__init__.py:51} INFO - Using executor SequentialExecutor
[2019-04-29 07:49:36,159] {base_task_runner.py:101} INFO - Job 102: Subtask task2 [2019-04-29 07:49:36,159] {models.py:273} INFO - Filling up the DagBag from /home/mjcarleb/HESWork/CSCI-E29/2019sp-airflow_project-mjcarleb/dags/AWS_Challenge.py
[2019-04-29 07:49:36,193] {base_task_runner.py:101} INFO - Job 102: Subtask task2 [2019-04-29 07:49:36,193] {cli.py:520} INFO - Running <TaskInstance: s3_badcred_sensor.task2 2019-04-28T00:00:00+00:00 [running]> on host localhost.localdomain
[2019-04-29 07:49:36,199] {bash_operator.py:77} INFO - Tmp dir root location: 
 /tmp
[2019-04-29 07:49:36,200] {bash_operator.py:86} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=s3_badcred_sensor
AIRFLOW_CTX_TASK_ID=task2
AIRFLOW_CTX_EXECUTION_DATE=2019-04-28T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-04-28T00:00:00+00:00
[2019-04-29 07:49:36,200] {bash_operator.py:100} INFO - Temporary script location: /tmp/airflowtmp9rvfunyu/task2l1rcv35p
[2019-04-29 07:49:36,200] {bash_operator.py:110} INFO - Running command: echo a big hadoop job putting files on s3
[2019-04-29 07:49:36,203] {bash_operator.py:119} INFO - Output:
[2019-04-29 07:49:36,204] {bash_operator.py:123} INFO - a big hadoop job putting files on s3
[2019-04-29 07:49:36,204] {bash_operator.py:127} INFO - Command exited with return code 0
[2019-04-29 07:49:36,218] {base_task_runner.py:101} INFO - Job 102: Subtask task2 /home/mjcarleb/anaconda3/envs/airflow/lib/python3.5/site-packages/airflow/utils/helpers.py:356: DeprecationWarning: Importing 'BashOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
[2019-04-29 07:49:36,218] {base_task_runner.py:101} INFO - Job 102: Subtask task2   DeprecationWarning)
[2019-04-29 07:49:40,566] {logging_mixin.py:95} INFO - [2019-04-29 07:49:40,565] {jobs.py:2527} INFO - Task exited with return code 0
